{
  "slug": "ethical-guidelines",
  "title": "Ethical Guidelines for the Sustainable Newsroom",
  "content": "As AI systems become embedded in reporting, editing, distribution and audience interaction, the central ethical challenge for journalism is no longer whether to use AI, but how to use it without surrendering responsibility, credibility or source protection.\nThere is another issue which is linked to ethical AI and that is striving for independence from large tech companies. As you can imagine, this is easier said than done. Where does financial dependence on a certain platform start to infringe on your ethical standing.\nThese guidelines are designed for a sovereign newsroom. In short, this means that one treats AI as infrastructure, not authority. You need to think of automation as assistance, not authorship and efficiency as a means to strengthen accountability, not weaken it.\nThe principles below establish non-negotiable red lines for editorial control, integrity and data security. In this context, editorial control means that humans and not tools, platforms, or algorithms retain final authority over journalistic decisions. Having a bedrock in the ethical principles is good preparation for when selecting and deciding on AI tools.\n## 1. The \"Human Command\" Principle\nThe Rule: AI is a tool for suggestion, never for decision. No AI output is published without human verification.\nThe \"Human-in-the-Loop\" workflow:\n○ Drafting vs. Publishing: AI may generate 100 headline ideas (Drafting), but a human editor must select the one that is accurate and fair (Publishing).\n○ The \"10-Second\" Audit: For every AI-generated summary or translation, the journalist must trace at least three claims back to the original source text. If the AI cannot provide a citation, the claim is treated as false until proven otherwise.\n○ The Kill Switch: Human editors must retain the technical ability to immediately retract or modify AI-driven content (automated chatbots) if they begin hallucinating or are hijacked by bad actors.\nTip: Ask students to treat AI like a \"Junior Intern\" who is incredibly fast but prone to pathological lying. You would never publish the intern's work unread so then do not publish the AI's.\n## 2. No \"Synthetic Reality\"\nThe Rule: Never use AI to simulate the physical reality of news events.\nIn conflict zones a photograph is evidence of a war crime. Using AI to generate \"realistic\" images of protests, soldiers, or destruction—even for \"illustration\"—corrodes the public's ability to believe real photos.\nProhibited Uses (Red Lines):\n○ Generating images of specific people (politicians, activists) doing things they never did.\n○ Generating \"photos\" of events (protests, bombings, meetings).\n○ Altering the content of real photos (for example, remove a person from a meeting).\nPermitted Uses (Green Lines):\n○ Abstract Concepts: Visualizing non-physical stories like \"cyberwarfare,\" \"inflation,\" or \"mental health\" where no specific reality is being claimed.\n○ Clearly Stylized Art: Using AI to create cartoons or sketches that obviously look like art, not photography.\nBy flooding the zone with fake realistic images, we give dictators an excuse to dismiss real evidence as \"just AI.\" Journalists must not contribute to this.\n## 3. The \"Zero Trust\" Data Rule\nThe Rule: Assume every cloud tool is compromised. Protect your sources from your software.\nNewsrooms can face high-level state surveillance. Their Terms of Service often allow them to review user data for \"safety\" or training purposes.\n- Green Data (Public Info): Press releases, published articles, public speeches.\n- Yellow Data (Internal/Low Risk): Drafts of opinion pieces, planning documents, non-sensitive interviews.\n- Red Data (Sensitive/Source Material): Whistleblower documents, interviews with soldiers/victims and unreleased corruption evidence."
}