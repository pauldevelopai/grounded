{
  "slug": "addendum-c",
  "title": "Futureproofing & Updating the Curriculum",
  "content": "The AI Toolkit is intentionally not a fixed list of tools. AI systems change frequently. Features are added or removed, pricing models shift, companies are acquired and data-use policies evolve. This addendum provides a practical, low-burden approach to curriculum maintenance that does not require technical expertise.\n## 1. A Layered Approach to Curriculum Updates\nRather than constant monitoring, educators are encouraged to use a layered update cycle that balances awareness with sustainability.\n## Ongoing informal awareness\nEducators should maintain general awareness of changes in the AI and journalism landscape through everyday signals:\n● Colleagues mentioning new tools or features\n● Students referencing emerging platforms\n● Headlines, social media posts, or conference discussions\nThese informal cues are often the earliest indicators that something has changed. At this stage, no action is required beyond noticing.\n## Monthly\nOnce per month, educators should conduct a short review focused on the tools actively referenced in their teaching. Key questions include:\n● Have any tools changed pricing, access levels, or ownership?\n● Have any tools been discontinued, merged, or significantly altered?\n● Have there been notable changes to data-use or privacy policies?\nSome institutions now formalize this as a brief \"tool sanity check\" to catch disruptions before they affect students.\n## Annual deep review\nOnce per year, a deeper review should be undertaken to reassess the toolkit as a whole. This is the appropriate moment to:\n● Retire tools that are obsolete, unstable, or no longer trustworthy\n● Introduce tools that have matured and proven their value\n● Revisit ethical, legal, and data-protection considerations\n● Update warnings, caveats, or teaching notes where needed\n## 2. Tracking Tools and Trends Responsibly\nEducators do not need to track the entire AI ecosystem. The focus should remain on tools that serve journalistic work and align with editorial values.\n## Example of a tool that compares LLMs\nLLM Arena (also known as LMArena, formerly Chatbot Arena) is a public, crowd-sourced platform for evaluating and comparing large language models (LLMs). On the platform, users can submit the same prompt to two anonymous AI models and vote on which response they think is better. These pairwise comparisons are aggregated into a leaderboard using an Elo-style rating system, allowing models to be ranked by performance based on real human preference data rather than just static metrics.\n## Monitoring tool stability\nSimple habits are sufficient:\n● Subscribe to one or two general AI or journalism newsletters\n● Set basic alerts for the names of tools used in class\n● Bookmark official blogs or status pages for key tools and review them periodically\nIf a tool loses functionality, changes its business model, or disappears, it should be reassessed or removed from teaching materials.\n## Evaluating new tools\nWhen a new AI tool comes to attention, educators should assess it using three criteria:\n1. Usability. Can students and educators learn it quickly?\n2. Relevance. Does it support real journalistic tasks such as research, writing, editing, or verification?\n3. Responsibility. Does it respect privacy, consent, accuracy, and transparency?\nEducators are encouraged to test tools briefly using classroom-relevant examples rather than relying on marketing claims. Popularity alone is not a reason to include a tool.\n## Maintaining trust and compliance\nEducators should remain alert to:\n● Changes in how tools handle user data\n● New requirements for uploads, permissions, or third-party sharing\n● Shifts from free access to paid or restricted models\nIf a tool's data practices change in ways that conflict with classroom expectations, its use should be paused and reconsidered. Maintaining trust with students is essential."
}